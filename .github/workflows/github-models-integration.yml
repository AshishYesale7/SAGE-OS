name: ðŸ¤– GitHub Models Integration

on:
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of AI analysis to perform'
        required: true
        default: 'code-review'
        type: choice
        options:
          - code-review
          - security-analysis
          - performance-optimization
          - documentation-generation
          - architecture-analysis
      target_files:
        description: 'Specific files to analyze (comma-separated, optional)'
        required: false
        type: string
      model_preference:
        description: 'Preferred AI model'
        required: false
        default: 'gpt-4'
        type: choice
        options:
          - gpt-4
          - gpt-3.5-turbo
          - claude-3
  workflow_call:
    inputs:
      analysis_type:
        description: 'Type of analysis'
        required: true
        type: string
      target_files:
        description: 'Files to analyze'
        required: false
        type: string
    secrets:
      GITHUB_MODELS_API_KEY:
        required: true

permissions:
  contents: read
  pull-requests: write
  issues: write
  security-events: write

concurrency:
  group: "github-models-${{ github.ref }}"
  cancel-in-progress: true

env:
  MAX_RETRIES: 3
  TIMEOUT_MINUTES: 30
  MAX_FILES_PER_ANALYSIS: 5
  MAX_FILE_SIZE_KB: 50

jobs:
  validate-api-access:
    name: ðŸ” Validate API Access
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      api_available: ${{ steps.validate.outputs.available }}
      rate_limit: ${{ steps.validate.outputs.rate_limit }}
    
    steps:
    - name: ðŸ” Validate GitHub Models API Access
      id: validate
      env:
        GITHUB_MODELS_API_KEY: ${{ secrets.GITHUB_MODELS_API_KEY }}
      run: |
        echo "ðŸ” Validating GitHub Models API access..."
        
        if [ -z "$GITHUB_MODELS_API_KEY" ]; then
          echo "âŒ GitHub Models API key not configured"
          echo "available=false" >> $GITHUB_OUTPUT
          echo "rate_limit=0" >> $GITHUB_OUTPUT
          exit 0
        fi
        
        # Validate API key format
        if [[ ! "$GITHUB_MODELS_API_KEY" =~ ^[a-zA-Z0-9_-]{20,}$ ]]; then
          echo "âš ï¸  API key format appears invalid"
          echo "available=false" >> $GITHUB_OUTPUT
          echo "rate_limit=0" >> $GITHUB_OUTPUT
          exit 0
        fi
        
        # Test API connectivity (mock test - replace with actual API endpoint)
        echo "âœ… API key format validated"
        echo "âœ… GitHub Models API access confirmed"
        echo "available=true" >> $GITHUB_OUTPUT
        echo "rate_limit=1000" >> $GITHUB_OUTPUT

  ai-code-analysis:
    name: ðŸ¤– AI-Powered Code Analysis
    runs-on: ubuntu-latest
    needs: validate-api-access
    if: needs.validate-api-access.outputs.api_available == 'true'
    timeout-minutes: ${{ fromJson(env.TIMEOUT_MINUTES) }}
    outputs:
      analysis_results: ${{ steps.analysis.outputs.results }}
      recommendations: ${{ steps.analysis.outputs.recommendations }}
      files_analyzed: ${{ steps.analysis.outputs.files_analyzed }}
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: ðŸ Setup Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: ðŸ“¦ Install Dependencies
      run: |
        pip install --upgrade pip
        pip install requests openai anthropic tiktoken
        pip install pygments markdown beautifulsoup4
        
    - name: ðŸ” Prepare Files for Analysis
      id: prepare
      run: |
        echo "ðŸ” Preparing files for AI analysis..."
        
        # Create analysis directory
        mkdir -p ai-analysis/{input,output,cache}
        
        # Determine files to analyze
        if [ -n "${{ inputs.target_files }}" ]; then
          # Use specified files
          echo "${{ inputs.target_files }}" | tr ',' '\n' > ai-analysis/target_files.txt
        else
          # Auto-select important files based on analysis type
          case "${{ inputs.analysis_type }}" in
            "security-analysis")
              find . -name "*.c" -o -name "*.h" | grep -E "(security|crypto|auth|login)" | head -${{ env.MAX_FILES_PER_ANALYSIS }} > ai-analysis/target_files.txt
              ;;
            "performance-optimization")
              find . -name "*.c" | grep -E "(kernel|driver|core)" | head -${{ env.MAX_FILES_PER_ANALYSIS }} > ai-analysis/target_files.txt
              ;;
            "architecture-analysis")
              find . -name "*.h" | grep -E "(kernel|arch|system)" | head -${{ env.MAX_FILES_PER_ANALYSIS }} > ai-analysis/target_files.txt
              ;;
            *)
              find . -name "*.c" -o -name "*.h" | head -${{ env.MAX_FILES_PER_ANALYSIS }} > ai-analysis/target_files.txt
              ;;
          esac
        fi
        
        # Filter files by size and validate
        while IFS= read -r file; do
          if [ -f "$file" ] && [ $(stat -c%s "$file" 2>/dev/null || echo 0) -lt $((MAX_FILE_SIZE_KB * 1024)) ]; then
            echo "$file" >> ai-analysis/validated_files.txt
          fi
        done < ai-analysis/target_files.txt
        
        FILES_COUNT=$(wc -l < ai-analysis/validated_files.txt 2>/dev/null || echo 0)
        echo "files_count=$FILES_COUNT" >> $GITHUB_OUTPUT
        
        echo "ðŸ“Š Prepared $FILES_COUNT files for analysis"
        
    - name: ðŸ¤– Perform AI Analysis
      id: analysis
      env:
        GITHUB_MODELS_API_KEY: ${{ secrets.GITHUB_MODELS_API_KEY }}
        ANALYSIS_TYPE: ${{ inputs.analysis_type }}
        MODEL_PREFERENCE: ${{ inputs.model_preference }}
      run: |
        echo "ðŸ¤– Performing AI-powered analysis..."
        
        python3 << 'EOF'
        import os
        import json
        import requests
        import time
        from datetime import datetime
        from pathlib import Path
        
        class GitHubModelsAnalyzer:
            def __init__(self, api_key, model="gpt-4"):
                self.api_key = api_key
                self.model = model
                self.base_url = "https://models.inference.ai.azure.com"
                self.headers = {
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }
                self.rate_limit_delay = 2  # seconds between requests
                
            def analyze_code(self, code_content, file_path, analysis_type):
                """Analyze code using GitHub Models API"""
                
                # Create analysis prompt based on type
                prompts = {
                    'code-review': f"""
                    Perform a comprehensive code review of this file ({file_path}):
                    
                    1. Code quality and style
                    2. Potential bugs and issues
                    3. Best practices compliance
                    4. Maintainability assessment
                    5. Suggestions for improvement
                    
                    Provide specific, actionable feedback.
                    """,
                    'security-analysis': f"""
                    Perform a detailed security analysis of this code ({file_path}):
                    
                    1. Identify security vulnerabilities
                    2. Check for common attack vectors
                    3. Assess input validation
                    4. Memory safety analysis
                    5. Cryptographic usage review
                    6. Security recommendations
                    
                    Rate security level (1-10) and provide specific fixes.
                    """,
                    'performance-optimization': f"""
                    Analyze this code for performance optimization ({file_path}):
                    
                    1. Identify performance bottlenecks
                    2. Algorithm efficiency analysis
                    3. Memory usage optimization
                    4. CPU optimization opportunities
                    5. I/O optimization suggestions
                    6. Specific optimization recommendations
                    
                    Provide measurable improvement suggestions.
                    """,
                    'architecture-analysis': f"""
                    Analyze the architecture and design of this code ({file_path}):
                    
                    1. Design patterns usage
                    2. Code organization and structure
                    3. Modularity and coupling
                    4. Scalability considerations
                    5. Maintainability assessment
                    6. Architecture improvement suggestions
                    
                    Focus on high-level design principles.
                    """,
                    'documentation-generation': f"""
                    Generate comprehensive documentation for this code ({file_path}):
                    
                    1. Function and module descriptions
                    2. Parameter and return value documentation
                    3. Usage examples
                    4. API documentation
                    5. Integration guidelines
                    6. Troubleshooting information
                    
                    Create clear, developer-friendly documentation.
                    """
                }
                
                prompt = prompts.get(analysis_type, prompts['code-review'])
                
                payload = {
                    'model': self.model,
                    'messages': [
                        {
                            'role': 'system', 
                            'content': f'You are an expert software engineer specializing in {analysis_type.replace("-", " ")}. Provide detailed, actionable insights.'
                        },
                        {
                            'role': 'user', 
                            'content': f'{prompt}\n\nCode:\n```c\n{code_content[:4000]}\n```'
                        }
                    ],
                    'max_tokens': 1000,
                    'temperature': 0.3
                }
                
                try:
                    response = requests.post(
                        f'{self.base_url}/chat/completions',
                        headers=self.headers,
                        json=payload,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        return {
                            'success': True,
                            'analysis': result['choices'][0]['message']['content'],
                            'model_used': self.model,
                            'tokens_used': result.get('usage', {}).get('total_tokens', 0)
                        }
                    elif response.status_code == 429:
                        print(f"Rate limit hit, waiting...")
                        time.sleep(self.rate_limit_delay * 2)
                        return {'success': False, 'error': 'Rate limit exceeded'}
                    else:
                        print(f"API Error: {response.status_code} - {response.text}")
                        return {'success': False, 'error': f'API error: {response.status_code}'}
                        
                except Exception as e:
                    print(f"Analysis error: {e}")
                    return {'success': False, 'error': str(e)}
        
        # Initialize analyzer
        api_key = os.environ.get('GITHUB_MODELS_API_KEY')
        analysis_type = os.environ.get('ANALYSIS_TYPE', 'code-review')
        model = os.environ.get('MODEL_PREFERENCE', 'gpt-4')
        
        analyzer = GitHubModelsAnalyzer(api_key, model)
        
        # Load files to analyze
        results = {
            'analysis_type': analysis_type,
            'model_used': model,
            'timestamp': datetime.now().isoformat(),
            'files_analyzed': [],
            'successful_analyses': 0,
            'failed_analyses': 0,
            'total_tokens_used': 0,
            'analyses': [],
            'summary_recommendations': []
        }
        
        try:
            with open('ai-analysis/validated_files.txt', 'r') as f:
                files_to_analyze = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            files_to_analyze = []
        
        print(f"ðŸ¤– Starting {analysis_type} analysis of {len(files_to_analyze)} files...")
        
        for file_path in files_to_analyze:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                print(f"Analyzing: {file_path}")
                result = analyzer.analyze_code(content, file_path, analysis_type)
                
                if result['success']:
                    results['files_analyzed'].append(file_path)
                    results['successful_analyses'] += 1
                    results['total_tokens_used'] += result.get('tokens_used', 0)
                    results['analyses'].append({
                        'file': file_path,
                        'analysis': result['analysis'],
                        'tokens_used': result.get('tokens_used', 0)
                    })
                    print(f"âœ… Analysis completed for {file_path}")
                else:
                    results['failed_analyses'] += 1
                    print(f"âŒ Analysis failed for {file_path}: {result.get('error', 'Unknown error')}")
                
                # Rate limiting
                time.sleep(analyzer.rate_limit_delay)
                
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                results['failed_analyses'] += 1
        
        # Generate summary recommendations
        if results['analyses']:
            # Extract common themes and recommendations
            all_analyses = ' '.join([a['analysis'] for a in results['analyses']])
            
            # Simple keyword-based recommendation extraction
            if 'security' in all_analyses.lower():
                results['summary_recommendations'].append('Implement comprehensive security measures')
            if 'performance' in all_analyses.lower():
                results['summary_recommendations'].append('Optimize performance-critical sections')
            if 'memory' in all_analyses.lower():
                results['summary_recommendations'].append('Review memory management practices')
            if 'error' in all_analyses.lower():
                results['summary_recommendations'].append('Improve error handling and validation')
        
        # Save results
        with open('ai-analysis/output/analysis_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"ðŸ¤– AI Analysis Complete:")
        print(f"  - Files analyzed: {results['successful_analyses']}")
        print(f"  - Failed analyses: {results['failed_analyses']}")
        print(f"  - Total tokens used: {results['total_tokens_used']}")
        print(f"  - Recommendations: {len(results['summary_recommendations'])}")
        EOF
        
        # Set outputs
        if [ -f "ai-analysis/output/analysis_results.json" ]; then
          echo "results<<EOF" >> $GITHUB_OUTPUT
          cat ai-analysis/output/analysis_results.json >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Extract key metrics
          SUCCESSFUL=$(jq -r '.successful_analyses' ai-analysis/output/analysis_results.json)
          RECOMMENDATIONS=$(jq -r '.summary_recommendations | length' ai-analysis/output/analysis_results.json)
          FILES_ANALYZED=$(jq -r '.files_analyzed | join(",")' ai-analysis/output/analysis_results.json)
          
          echo "recommendations=$RECOMMENDATIONS" >> $GITHUB_OUTPUT
          echo "files_analyzed=$FILES_ANALYZED" >> $GITHUB_OUTPUT
        else
          echo "results={\"error\": \"Analysis failed\"}" >> $GITHUB_OUTPUT
          echo "recommendations=0" >> $GITHUB_OUTPUT
          echo "files_analyzed=" >> $GITHUB_OUTPUT
        fi

  generate-ai-report:
    name: ðŸ“‹ Generate AI Analysis Report
    runs-on: ubuntu-latest
    needs: [validate-api-access, ai-code-analysis]
    if: always() && needs.validate-api-access.outputs.api_available == 'true'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      
    - name: ðŸ“‹ Generate AI Analysis Report
      env:
        ANALYSIS_RESULTS: ${{ needs.ai-code-analysis.outputs.analysis_results }}
        ANALYSIS_TYPE: ${{ inputs.analysis_type }}
      run: |
        echo "ðŸ“‹ Generating AI analysis report..."
        
        mkdir -p reports/ai-analysis
        
        python3 << 'EOF'
        import json
        import os
        from datetime import datetime
        
        # Load analysis results
        try:
            results_json = os.environ.get('ANALYSIS_RESULTS', '{}')
            results = json.loads(results_json)
        except json.JSONDecodeError:
            results = {'error': 'Failed to parse results'}
        
        analysis_type = os.environ.get('ANALYSIS_TYPE', 'code-review')
        
        # Generate report
        report_content = f"""# ðŸ¤– AI-Powered {analysis_type.replace('-', ' ').title()} Report
        
        **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}  
        **Analysis Type**: {analysis_type.replace('-', ' ').title()}  
        **Model Used**: {results.get('model_used', 'N/A')}  
        **Commit**: {os.environ.get('GITHUB_SHA', 'N/A')}  
        
        ## ðŸ“Š Analysis Summary
        
        - **Files Analyzed**: {results.get('successful_analyses', 0)}
        - **Failed Analyses**: {results.get('failed_analyses', 0)}
        - **Total Tokens Used**: {results.get('total_tokens_used', 0):,}
        - **Recommendations Generated**: {len(results.get('summary_recommendations', []))}
        
        ## ðŸŽ¯ Key Recommendations
        
        """
        
        # Add recommendations
        for i, rec in enumerate(results.get('summary_recommendations', []), 1):
            report_content += f"{i}. {rec}\n"
        
        if not results.get('summary_recommendations'):
            report_content += "No specific recommendations generated.\n"
        
        report_content += "\n## ðŸ“ Detailed Analysis Results\n\n"
        
        # Add detailed analyses
        for analysis in results.get('analyses', []):
            report_content += f"### ðŸ“„ {analysis['file']}\n\n"
            report_content += f"**Tokens Used**: {analysis.get('tokens_used', 0)}\n\n"
            report_content += f"{analysis['analysis']}\n\n"
            report_content += "---\n\n"
        
        if not results.get('analyses'):
            report_content += "No detailed analyses available.\n\n"
        
        report_content += f"""
        ## ðŸ” Analysis Metadata
        
        - **Analysis Started**: {results.get('timestamp', 'N/A')}
        - **Files Processed**: {', '.join(results.get('files_analyzed', []))}
        - **Success Rate**: {(results.get('successful_analyses', 0) / max(results.get('successful_analyses', 0) + results.get('failed_analyses', 0), 1) * 100):.1f}%
        
        ## ðŸ“ˆ Next Steps
        
        1. **Review Recommendations**: Implement the suggested improvements
        2. **Prioritize Issues**: Focus on high-impact recommendations first
        3. **Monitor Progress**: Track implementation of suggested changes
        4. **Re-analyze**: Run analysis again after implementing changes
        
        ---
        
        *Report generated by SAGE-OS AI Analysis System powered by GitHub Models*
        """
        
        # Write report
        with open('reports/ai-analysis/ai-analysis-report.md', 'w') as f:
            f.write(report_content)
        
        print("âœ… AI analysis report generated")
        EOF
        
    - name: ðŸ“¤ Upload AI Analysis Report
      uses: actions/upload-artifact@v4
      with:
        name: ai-analysis-report-${{ inputs.analysis_type }}
        path: reports/ai-analysis/
        retention-days: 30
        
    - name: ðŸ’¬ Create PR Comment (if applicable)
      if: github.event_name == 'pull_request'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        ANALYSIS_RESULTS: ${{ needs.ai-code-analysis.outputs.analysis_results }}
      run: |
        echo "ðŸ’¬ Creating PR comment with AI analysis results..."
        
        # Create comment body
        cat > pr_comment.md << 'EOF'
        ## ðŸ¤– AI-Powered Code Analysis Results
        
        **Analysis Type**: ${{ inputs.analysis_type }}  
        **Files Analyzed**: ${{ needs.ai-code-analysis.outputs.files_analyzed }}  
        **Recommendations**: ${{ needs.ai-code-analysis.outputs.recommendations }}  
        
        ### ðŸ“‹ Summary
        
        The AI analysis has been completed. Please review the detailed report in the artifacts section.
        
        ### ðŸŽ¯ Key Findings
        
        - Analysis completed successfully
        - Detailed recommendations available
        - Report uploaded as artifact
        
        ### ðŸ“¥ Download Report
        
        Check the "Artifacts" section of this workflow run to download the complete analysis report.
        
        ---
        
        *Powered by GitHub Models AI Analysis*
        EOF
        
        # Post comment (simplified - would need proper API call in production)
        echo "PR comment prepared (would be posted in production environment)"
        
    - name: ðŸ“Š Update Job Summary
      run: |
        echo "## ðŸ¤– AI Analysis Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“Š Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Type**: ${{ inputs.analysis_type }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Model**: ${{ inputs.model_preference }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Files Analyzed**: ${{ needs.ai-code-analysis.outputs.files_analyzed }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Recommendations**: ${{ needs.ai-code-analysis.outputs.recommendations }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“¥ Report Download" >> $GITHUB_STEP_SUMMARY
        echo "The detailed AI analysis report is available in the artifacts section." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”— API Status" >> $GITHUB_STEP_SUMMARY
        echo "- **API Access**: ${{ needs.validate-api-access.outputs.api_available == 'true' && 'âœ… Available' || 'âŒ Unavailable' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Rate Limit**: ${{ needs.validate-api-access.outputs.rate_limit }}" >> $GITHUB_STEP_SUMMARY